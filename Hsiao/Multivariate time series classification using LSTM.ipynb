{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e45c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pytorch_lightning as pl\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import cluster, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data_utils\n",
    "import torch.nn.functional as Func\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b534db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "f = pd.read_csv(\"data.csv\")\n",
    "f.rename(columns={'time sample band g': 'time_g', 'time sample band r': 'time_r',\n",
    "                    'time sample band i': 'time_i','total flux + noise band g': 'tfnbg',\n",
    "                    'total flux + noise band r': 'tfnbr', 'total flux + noise band i': 'tfnbi',}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3c89d6",
   "metadata": {},
   "source": [
    "def standard(dataset):\n",
    "    \n",
    "    scaler = MinMaxScaler(feature_range = (0, 1), copy=False)\n",
    "    #scaler = StandardScaler()\n",
    "    \n",
    "    df = 0\n",
    "    dfbis = 0\n",
    "    for ide, group in dataset.groupby('ID'):\n",
    "\n",
    "        a = dataset[dataset.ID == ide]\n",
    "        c = a[['ID', 'images']]\n",
    "        data = a[a.columns[2:]].copy()\n",
    "        t = ['time_g', 'time_r', 'time_i']\n",
    "        \n",
    "        data[['tfnbg', 'tfnbr', 'tfnbi']] = scaler.fit_transform(data[['tfnbg', 'tfnbr', 'tfnbi']])\n",
    "        data[t] = data[t]-np.min(data[t])\n",
    "\n",
    "        if ide == 0:\n",
    "            df = pd.concat([c, data], axis=1)\n",
    "        else:\n",
    "            dfbis = pd.concat([c, data], axis = 1)\n",
    "            df = pd.concat([df, dfbis])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfabebb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "f['images'] = f['images'].replace([1, 2, 3, 4], [0, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079732fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = f[:91*8000]\n",
    "val = f[91*8000:91*9000]\n",
    "test = f[91*9000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cc9e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((train['images']==0).sum())\n",
    "print((train['images']==1).sum())\n",
    "\n",
    "\n",
    "print((test['images']==0).sum())\n",
    "print((test['images']==1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfcbe36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fb0aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_to_dataset(f):\n",
    "        \n",
    "    scaler = MinMaxScaler(feature_range = (0, 1), copy=False)\n",
    "    #scaler = StandardScaler()\n",
    "\n",
    "    features_columns = f.columns.tolist()[3::2]\n",
    "    T = []\n",
    "    F = []\n",
    "    \n",
    "    for ide, group in f.groupby('ID'):\n",
    "\n",
    "        a = f[f.ID == ide]\n",
    "        c = a[['ID', 'images']]\n",
    "        data = a.copy()\n",
    "        #t = ['time_g', 'time_r', 'time_i']\n",
    "        \n",
    "        data[['tfnbg', 'tfnbr', 'tfnbi']] = scaler.fit_transform(data[['tfnbg', 'tfnbr', 'tfnbi']])\n",
    "        #data[t] = data[t]-np.min(data[t])\n",
    "\n",
    "        T.append(a['images'].values[0])\n",
    "        F.append(torch.tensor(data[features_columns].T.values)[None])\n",
    "\n",
    "    \n",
    "    T = torch.tensor(T)\n",
    "    F = torch.cat(F, dim=0)\n",
    "    print(F.shape)\n",
    "    return TensorDataset(F.double(), T.double())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127a89a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainSet = dataframe_to_dataset(train)\n",
    "TestSet = dataframe_to_dataset(test)\n",
    "ValSet = dataframe_to_dataset(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cc393c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b5c834",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_channels, n_classes, n_hidden=64):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(3, 5, kernel_size=(5,))\n",
    "        self.pool = nn.MaxPool1d(2, 2)\n",
    "        self.conv2 = nn.Conv1d(5,20, kernel_size=(5,))\n",
    "        #self.lstm1 = nn.Conv1d(\n",
    "        #in_channels = n_features,\n",
    "        #out_channels = n_hidden,\n",
    "        #kernel_size = 5)\n",
    "        #self.lstm1 = nn.LSTM(\n",
    "        #    input_size = n_channels,\n",
    "        #    hidden_size = n_hidden,\n",
    "        #    num_layers = 5) \n",
    "        \n",
    "        #self.lstm2 = nn.Conv1d(\n",
    "        #in_channels = n_hidden,\n",
    "        #out_channels = 128,\n",
    "        #kernel_size = 5)\n",
    "        \n",
    "        \n",
    "        self.c1 = nn.Linear(380, 64)\n",
    "        self.c2 = nn.Linear(64, 10)\n",
    "        self.c3 = nn.Linear(10, n_classes-1)\n",
    "            \n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x, (ht, ct) = self.lstm1(x) \n",
    "        x = self.pool(Func.relu(self.conv1(x)))\n",
    "        x = self.pool(Func.relu(self.conv2(x)))\n",
    "        #x = self.lstm1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x= self.c1(x)\n",
    "        x = self.c2(x)\n",
    "        pred = self.c3(x)\n",
    "        #pred = torch.sigmoid(pred)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03388b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc3c89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SequenceModel(n_channels = 3, \n",
    "                        n_classes = 2)\n",
    "model = model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd4abac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ffa9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainLoader = data_utils.DataLoader(TrainSet, batch_size = 16, shuffle = True)\n",
    "ValLoader = data_utils.DataLoader(ValSet, batch_size = 16, shuffle = False)\n",
    "TestLoader = data_utils.DataLoader(TestSet, batch_size = 16, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cb8502",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de22a684",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8534f1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- State_dict du model : ---\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc4d2a3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "losses = []  \n",
    "\n",
    "N_epochs = 10\n",
    "n_verbose = 50\n",
    "smooth_loss = []\n",
    "accuracy = 0\n",
    "acc_loss = []\n",
    "\n",
    "for epoch in range(N_epochs):  # Loop over epochs\n",
    "    print(f'Running epoch {epoch+1}')\n",
    "    running_loss = 0.0\n",
    "    for i, (features, label) in enumerate(TrainLoader):\n",
    "        #print(features.shape)\n",
    "        # Erase previous gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #print(features.shape)\n",
    "        labels_pred =[]\n",
    "        # Forward Propagation \n",
    "        labels_pred = model(features)  \n",
    "        #print(labels_pred.shape)\n",
    "        \n",
    "        #print(labels_pred)\n",
    "        #label = tensor(np.array(label[0].item()))   # for size 2 in output of the model\n",
    "        #label = torch.tensor([label[0].item()]).to(torch.float)\n",
    "        \n",
    "        \n",
    "        # Loss computation\n",
    "        loss = loss_function(labels_pred, label[..., None])\n",
    "        #val_loss = loss_function(labels_val_pred, labels_val)\n",
    "        # Save loss for future analysis\n",
    "        losses.append(loss.item())\n",
    "        #val_losses.append(val_loss)\n",
    "        \n",
    "        # Compute gradients (backpropagation)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Weight update\n",
    "        optimizer.step()\n",
    "        # ACcuracy:\n",
    "        labels_pred[labels_pred<0.5] = 0\n",
    "        labels_pred[labels_pred>=0.5] = 1\n",
    "\n",
    "        accuracy = torch.add(accuracy, torch.sum(labels_pred-label))\n",
    "        \n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        losses.append(loss.item())\n",
    "        if i % n_verbose == n_verbose-1:    \n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / n_verbose:.5f}, accuracy: {accuracy/n_verbose:.5f}')\n",
    "            smooth_loss.append(running_loss / n_verbose)\n",
    "            acc_loss.append(accuracy.detach().numpy()/n_verbose)\n",
    "            running_loss = 0.0\n",
    "            accuracy = 0.\n",
    "    plt.plot(np.log10(np.array(losses)))\n",
    "    plt.plot(np.arange(len(smooth_loss))*n_verbose*2+n_verbose/2., np.log10(np.array(smooth_loss)))\n",
    "    #plt.ylim([0,1])\n",
    "    plt.show()\n",
    "    plt.plot(np.arange(len(smooth_loss))*n_verbose*2+n_verbose/2., np.array(acc_loss))\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d452de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display loss evolution\n",
    "fig, axes = plt.subplots(figsize=(8,6))\n",
    "axes.plot(losses,'r-',lw=2,label='Training loss function')\n",
    "plt.plot(np.arange(len(smooth_loss))*n_verbose*2+n_verbose/2., (np.array(smooth_loss)), label = 'smoothed loss')\n",
    "#axes.plot(val_losses,'b-',lw=2,label='Validation loss function')\n",
    "axes.set_xlabel('N iterations',fontsize=18)\n",
    "axes.set_ylabel('Loss',fontsize=18)\n",
    "#plt.ylim([0, 1])\n",
    "plt.legend(loc='upper right',fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33d0a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_losses = []\n",
    "for epoch in range(N_epochs):  # Loop over epochs\n",
    "    running_loss = 0.0\n",
    "   \n",
    "    for features, labels in ValLoader:\n",
    "        \n",
    "        # Forward Propagation \n",
    "        labels_pred = model(features.double())\n",
    "        label = torch.tensor([labels[0].item()]).to(torch.double)\n",
    "\n",
    "        # Loss computation\n",
    "        loss = loss_function(labels_pred, label)\n",
    "\n",
    "        # Save loss for future analysis\n",
    "        val_losses.append(loss.item())\n",
    "        \n",
    "    print('Training done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca08902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "def accuracy(model,dataloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # No need to compute gradients here\n",
    "    with torch.no_grad():\n",
    "        for features, labels in dataloader:\n",
    "            print(len(labels))\n",
    "            # Forward propagation to get predictions\n",
    "            pred = model(features.float()) \n",
    "            otherprob = 1 - pred.item()           \n",
    "            outputs = torch.tensor([[pred, otherprob]])\n",
    "            labels = torch.tensor([labels[0].item()]).to(torch.float)\n",
    "            print(labels)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            #print(total)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return 100.0 * correct / total       \n",
    "\n",
    "print('Accuracy : %.2f %%' % (accuracy(model,TestLoader)))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17fd48b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d45b47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
